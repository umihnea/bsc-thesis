\documentclass[12pt,twoside]{report}
\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage{amsmath,amssymb}
\usepackage[group-separator={,}]{siunitx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm,bindingoffset=6mm]{geometry}
\setlength{\headheight}{15pt}
\usepackage{sectsty}

% hyperref
% source: timmurphy.org/2014/03/11/latex-table-of-contents-with-clickable-links/
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    urlcolor=red,
    linktoc=all
}

% math commands
\newcommand\given[1][]{\:#1\vert\:}
\DeclareMathOperator*{\argmax}{\arg\!\max}

% fancyhdr
\usepackage{fancyhdr}
\newcommand{\changefont}{%
    \fontsize{9}{11}\selectfont
}
\pagestyle{fancy}
\fancyhead[LE,RO]{\changefont \slshape \rightmark} %section
\fancyhead[RE,LO]{\changefont \slshape \leftmark} %chapter

% graphicx
\usepackage{graphicx}
\graphicspath{{images/}}

% biblatex
\usepackage[square,numbers]{natbib}
\bibliographystyle{unsrtnat}

\usepackage[nottoc]{tocbibind}

% For blank pages
% solution detailed here https://tex.stackexchange.com/a/36881
\usepackage{afterpage}
\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\begin{document}

\begin{titlepage}
    \center % Center everything on the page

    % University, Faculty and Specialization
    {\scshape\LARGE Babeş-Bolyai University Cluj-Napoca \par}
    \vspace{0.125cm}
    {\scshape\LARGE Faculty of Mathematics and Computer Science\par}
    \vspace{0.125cm}
    {\scshape\LARGE Specialization Computer Science\par}
    \vspace{5cm}

    % Title section
    {
        \bfseries
        \LARGE \uppercase{Diploma Thesis} \\[1.5cm]
        \LARGE \uppercase{Using Deep Q-networks to learn Pac-Man}
    }\\[4cm]

    % Author and Supervisor
    \begin{flushleft}
        \Large
            \textbf{Supervisor}
            \vspace{0.2cm}\\
        \Large
            Lect. Ph.D. Radu D. Găceanu
            \vspace{0.125cm}\\
    \end{flushleft}

    \begin{flushright}
        \Large
            \textbf{Author}
            \vspace{0.2cm}\\
        \Large
            Mihnea Ungureanu
    \end{flushright}

    \vfill

    % Year
    {\center \large 2020}
\end{titlepage}

% A blank page is required
% The blankpage custom command is defined above 
\blankpage

% Romanian title page
\begin{titlepage}
    \center % Center everything on the page
    
    % University, Faculty and Specialization
    {\scshape\LARGE Universitatea Babeş-Bolyai Cluj-Napoca \par}
    \vspace{0.125cm}
    {\scshape\LARGE Facultatea de Matematică şi Informatică \par}
    \vspace{0.125cm}
    {\scshape\LARGE Specializarea Informatică Engleză \par}
    \vspace{5cm}
    
    % Title section
    {
        \bfseries
        \LARGE LUCRARE DE LICENȚĂ \\[1.5cm]
        \LARGE \uppercase{Folosirea de Deep Q-networks pentru a învĂȚa Pac-Man}
    }\\[4cm]
    
    % Author and Supervisor
    \begin{flushleft}
        \Large
            \textbf{Conducător științific}
            \vspace{0.2cm}\\
        \Large
            Lect. dr. Radu D. Găceanu
            \vspace{0.125cm}\\
    \end{flushleft}
    
    \begin{flushright}
        \Large
            \textbf{Absolvent}
            \vspace{0.2cm}\\
        \Large
            Mihnea Ungureanu
    \end{flushright}
    
    \vfill
    
    % Year
    {\center \large 2020}
    
\end{titlepage}

\blankpage

\chapter*{Abstract}
\small
The main purpose of this thesis is to implement, test and analyze the results of four fundamental algorithms in Deep Reinforcement Learning, in order to select the most performant approach at playing a variant of the famous arcade game \emph{Pac-Man}.
Our collection of implemented agents features a replication of DeepMind’s strategy, using Deep Q-networks (DQN), as well as two other performant successors to the original -- Double DQN, Prioritized Experience Replay (PER), and a hybrid of the latter two (Double PER).
A secondary aim is to build the necessary tools to study the strength of Pac-Man as a benchmark in Reinforcement Learning (RL), enabling others to collaborate and extend it.

% Despite seeming like a toy problem, learning to solve virtual environments provides insight into complex, real-world environments at an insignificant fraction of the cost of running realistic simulations.

The theoretical part of this thesis starts by exploring classical Reinforcement Learning techniques, then transitions to modern approaches which leverage Artificial Neural Networks (ANNs) to build more advanced agents, capable of solving more complex tasks.

The practical part consists of a Python framework for training, evaluating and benchmarking the performance of RL agents.
Our framework's main feature is the library of agents (software robots), actualizing the four aformentioned approaches.
Besides this, it also provides building blocks for training and evaluation logic, along with tools for visualizing and analyzing the results.
The main focus of the framework is on extensibility and ease of remote deployment.

For our evaluation, we start by training each agent by running game simulations for approximatively \num{120000} episodes per agent, tracking the score of each one for analysis.
After training is complete, each agent is again run for \num{2000} episodes to collect score data.
In our ranking, the top performing algorithms based on mean obtained score during evaluation are Double DQN and PER, scoring above the DQN baseline and several times over a random agent included for control.

Our contribution consists of reproducing and studying the performance of four fundamental algorithms in deep RL on the game of Pac-Man.
On one hand, providing our own rendition of the algorithms, based on their original specifications, and comparing our obtained performance with them, we study the reproducibility of the original papers.
On the other hand, our thesis introduces a novel perspective by focusing on depth over breadth.
Other works, including the original studies themselves, are explicitly focused on one-size-fits-all approaches and exclusively evaluate one algorithm across multiple games.
In our thesis, we reverse the perspective and instead rank several algorithms based on their performance in Pac-Man, which we believe is a suitable benchmark due to its highly dynamic nature.

We use a suite of high-performance libraries. The neural networks powering the agents are implemented using PyTorch, a powerful Python machine learning library based on Torch. The Atari toolkit from OpenAI Gym provides our Pac-Man environment, and we analyze our results using Jupyter Notebook, a well-established Python data science tool.

\hfill \break
This work is the result of my own activity. I have neither given nor received unauthorised assistance on this work.
\begin{flushright}
    Mihnea Ungureanu
\end{flushright}


% ToC placed between abstract and introduction
\tableofcontents


\chapter{Introduction}
Over the relatively recent years, Reinforcement Learning (RL) has gained immense popularity.
Stemming from the agent-based approach to artificial intelligence, it is a unique paradigm within the field of Machine Learning (ML).
RL directly focuses on building agents that make optimal decisions in an environment in order to maximize a notion of cumulative reward.
RL has a broad focus and researchers have trained RL agents to solve a variety of tasks.

One of the field's most notable achievments is DeepMind's \textbf{AlphaGo} family of Go-playing programs \cite{ago, alpha-zero}.
Beating humans at Go was widely acknowledged as the next frontier in game-playing AI after the defeat of Gary Kasparov in 1997\footnotemark.
Go was a natural progression from chess due to its larger state space and higher branching factor.
In 2016, AlphaGo became world renowned after defeating Lee Sedol, 18-times world champion, thus becoming the first computer system achieving \textbf{superhuman perfomance} in the game of Go.
\footnotetext{Gary Kasparov, the legendary chess champion, was defeated by IBM chess-playing computer DeepBlue in 1997, marking the first superhuman computer performance in chess.}

However, a multitude of interesting studies in RL focuses on agents learning in \textbf{video games} or other game-like environments.
In \emph{Mnih et al.}'s Atari DQN paper \cite{atari-dqn}, which is covered in this thesis, the DeepMind team proposes a method to train agents to play Atari video games exclusively from raw video input.
The study produced agents able to surpass human-level performance in Breakout, Enduro and Pong.
More importantly, it inspired a long chain of attempts of the research community to improve the original algorithm and beat the records it set.
More recent reasearch in Deep Reinforcement Learning targets more complex video games -- Starcraft II by DeepMind in \cite{starcraft-le-paper} and DoTA 2, targeted by OpenAI with OpenAI Five \cite{openai-five-paper}.

Following this trend, our thesis's main aim is to explore a challenging video game environment by testing the performance of the existing standards in Deep Reinforcement Learning.
We hope to discover potentially underexplored areas in the design of AI agents and to outline some limitations of the existing standards, in order to further the development of this field.

Our choice for an environment for studying agents is a variant of popular arcade video game \emph{Pac-Man}, developed and released by Namco Ltd. in 1980.
This choice is motivated by two factors: the potential challenges it poses to AI and its fun and intuitive appeal for humans.
The game is challenging for AI agents because it involves balancing the goal of exploration and point acquisition with escaping different classes of enemies that chase the player.

Toy problems such as playing video games from raw sensory input are the training ground which precedes real-world environments which require real-time intelligent control, such as self-driving cars.
Innovation in AI, as in all domains, begins at a small scale.

In order to accomplish our objective of benchmarking existing standards, we build an agent framework featuring the implementation of four major algorithms: the original DQN and three of its successors -- Double DQN, Prioritized Experience Replay (PER) and a hybrid of the latter two (Double PER).
The provided algorithms use the same model architecture, a convolutional neural network proposed in \cite{atari-dqn}, but each differs in the way it learns.
The framework additionally provides easily configurable training and evaluation logic, a solid \emph{Pac-Man} environment for experiments and intuitive yet easily extensible tools for results analysis.

Our framework is a crossover between existing concepts but is a completly new experiment platform.
It provides its own extensible agent implementations -- similar to OpenAI Baselines -- which are built interface with OpenAI Gym, but also includes its own tools for analysis, bridging the gap between design and evaluation.
Just as importantly, we combine that with flexible configurations, extensibility, Cloud-first deployment and PyTorch support (which by itself is generally limited among similar platforms).

This thesis can be roughly divided into a theoretical part which is compact and contained entirely within one exhaustive chapter, and a practical part, comprised of an Application chapter describing our framework followed by a section describing our experiments an analysis of their results.

\emph{Chapter \ref{chapter:background}, ``Background''} presents a comprehensive overview of the theory necessary to understanding reinforcement learning.
We start by introducing agent-based AI, then continue with a
section dedicated to understanding fundamental concepts in RL.
The second half of the chapter is dedicated to deep reinforcement learning, which can roughly be defined as combination of classic RL and neural networks.
Among the topics covered in the second half are convolutional neural networks (ConvNets) and some proeminent solution methods in deep RL.

\emph{Chapter \ref{chapter:practical}, ``Playing Pac-Man using Deep Q-learning''} formally states our problem, explains our chosen algorithms -- DQN, Double DQN and Prioritized Experience Replay (PER) -- and presents our framework's implementation.
We present an in-depth rundown of the structure of the framework, composed of a \texttt{common} module containing code shared amongst all agent implementations (including for training and evaluation), separate modules for each agent (the most important being \texttt{deepq} for DQN), and auxiliary modules -- agent subcomponents (in particular, the agent memory in \verb|replay_memory|), a model checkpoint manager, a sum tree implementation, etc.
Separate from the main code, we have a collection of interactive notebooks for plotting and analyzing results.

\emph{Chapter \ref{chapter:results}, ``Results and Evaluation''} defines evaluation in the context of RL, presents our methods of measuring the performance of the agents and analyzes the obtained data of our various experiments.
As a quick summary, we obtain results which confirm existing studies. Our highest ranking algorithms in with regard to mean score in Pac-Man are PER, followed closely by Double DQN.
Our analysis shows that both are an improvement over the original DQN and outperform a random agent by a considerable margin (up to $12$ times).

In our ending chapter, \emph{Chapter \ref{chapter:conclusion}, ``Conclustion and Future Work''}, we sum up our methods and results and we identify future areas of improvement. Additionally, we try to predict the direction of RL research based on our conclusions.

\chapter{Background} \label{chapter:background}
\input{chapters/background}

\chapter{Playing Pac-Man using Deep Q-learning} \label{chapter:practical}
\input{chapters/approach}

\chapter{Results and Evaluation} \label{chapter:results}
\input{chapters/results}

\chapter{Conclusion and Future Work} \label{chapter:conclusion}
\input{chapters/conclusion}

\bibliography{references}

\end{document}