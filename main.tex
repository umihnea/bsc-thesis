\documentclass[12pt,twoside]{report}
\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage{amsmath,amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm,bindingoffset=6mm]{geometry}
\setlength{\headheight}{15pt}

% hyperref
% source: timmurphy.org/2014/03/11/latex-table-of-contents-with-clickable-links/
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    urlcolor=red,
    linktoc=all
}

% math commands
\newcommand\given[1][]{\:#1\vert\:}
\DeclareMathOperator*{\argmax}{\arg\!\max}

% fancyhdr
\usepackage{fancyhdr}
\newcommand{\changefont}{%
    \fontsize{9}{11}\selectfont
}
\pagestyle{fancy}
\fancyhead[LE,RO]{\changefont \slshape \rightmark} %section
\fancyhead[RE,LO]{\changefont \slshape \leftmark} %chapter

% graphicx
\usepackage{graphicx}
\graphicspath{{images/}}

% biblatex
\usepackage[square,numbers]{natbib}
\bibliographystyle{unsrtnat}

\usepackage[nottoc]{tocbibind}

% For blank pages
% solution detailed here https://tex.stackexchange.com/a/36881
\usepackage{afterpage}
\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\begin{document}

\begin{titlepage}
    \center % Center everything on the page

    % University, Faculty and Specialization
    {\scshape\LARGE Babeş-Bolyai University Cluj-Napoca \par}
    \vspace{0.125cm}
    {\scshape\LARGE Faculty of Mathematics and Computer Science\par}
    \vspace{0.125cm}
    {\scshape\LARGE Specialization Computer Science\par}
    \vspace{5cm}

    % Title section
    {
        \bfseries
        \LARGE \uppercase{Diploma Thesis} \\[1.5cm]
        \huge Using Deep Q-networks to learn Pac-Man
    }\\[4cm]

    % Author and Supervisor
    \begin{flushleft}
        \Large
            \textbf{Supervisor}
            \vspace{0.2cm}\\
        \Large
            Lect. Ph.D. Radu D. Găceanu
            \vspace{0.125cm}\\
    \end{flushleft}

    \begin{flushright}
        \Large
            \textbf{Author}
            \vspace{0.2cm}\\
        \Large
            Mihnea Ungureanu
    \end{flushright}

    \vfill

    % Year
    {\center \large 2020}
\end{titlepage}

% A blank page is required
% The blankpage custom command is defined above 
\blankpage

% Romanian title page
\begin{titlepage}
    \center % Center everything on the page
    
    % University, Faculty and Specialization
    {\scshape\LARGE Universitatea Babeş-Bolyai Cluj-Napoca \par}
    \vspace{0.125cm}
    {\scshape\LARGE Facultatea de Matematică şi Informatică \par}
    \vspace{0.125cm}
    {\scshape\LARGE Specializarea Informatică Engleză \par}
    \vspace{5cm}
    
    % Title section
    {
        \bfseries
        \LARGE LUCRARE DE LICENȚĂ \\[1.5cm]
        \huge Folosind Deep Q-networks pentru a învăța Pac-Man
    }\\[4cm]
    
    % Author and Supervisor
    \begin{flushleft}
        \Large
            \textbf{Conducător științific}
            \vspace{0.2cm}\\
        \Large
            Lect. dr. Radu D. Găceanu
            \vspace{0.125cm}\\
    \end{flushleft}
    
    \begin{flushright}
        \Large
            \textbf{Absolvent}
            \vspace{0.2cm}\\
        \Large
            Mihnea Ungureanu
    \end{flushright}
    
    \vfill
    
    % Year
    {\center \large 2020}
    
\end{titlepage}

\blankpage

\chapter*{Abstract}
This thesis presents an autonomous agent capable of learning optimal behaviour in video game environments.

We focus on a variant of the famous arcade game \emph{Pac-Man}, due to its high potential as a research tool in the study of intelligent agents, as well as intuitive appeal for human players.

Despite seeming like a toy problem, learning to solve virtual environments provides insight into complex, real-world environments at an insignificant fraction of the cost of running realistic simulations.

The main purpose of this thesis is to implement, test and analyze results of different algorithms for autonomous learning and select the most performant approach at playing \emph{Pac-Man}.
A secondary aim is to test whether Pac-Man is a strong benchmark environment in \textbf{reinforcement learning (RL)} and search for challenges which would allow researchers to gain further insight into AI agents.

The theoretical part of this thesis starts by exploring classical \textbf{reinforcement learning} techniques, then transitions to modern approaches which leverage \textbf{artificial neural networks (ANNs)} as functional approximators for RL.

The practical part consists of multiple algorithm implementations, as well as a light framework for training, evaluating and benchmarking the performance of RL agents.
The provided agents include a replication of DeepMind’s \textbf{deep  Q-learning} strategy \cite{atari-dqn}, as well as more performant successors to the original paper: DDQN \cite{ddqn-paper} and Prioritized Experience Replay \cite{per-paper}.

The agents are implemented using PyTorch, a powerful Python machine learning library based on Torch. We use the Atari toolkit from OpenAI Gym, which was designed for RL and provides interfaces to keep track of in-game score, lives and other indicators.
Finally, we analyze our results using Jupyter Notebook, a well-established Python data science tool.

\hfill \break
This work is the result of my own activity. I have neither given nor received unauthorised assistance on this work.
\begin{flushright}
    Mihnea Ungureanu
\end{flushright}


% ToC placed between abstract and introduction
\tableofcontents


\chapter{Introduction}
Over the relatively recent years, \textbf{reinforcement learning (RL)} has gained immense popularity.
Stemming from the agent-based approach to artificial intelligence, it is a unique paradigm within the field of machine learning.
RL directly focuses on building agents that make optimal decisions in an environment in order to maximize a notion of cumulative reward.
RL has a broad focus and researchers have trained RL agents to solve a variety of tasks.

One of the field's most notable achievments is DeepMind's \textbf{AlphaGo} family of Go-playing programs \cite{ago, alpha-zero}.
Beating humans at Go was widely acknowledged as the next frontier in game-playing AI after the defeat of Gary Kasparov in 1997\footnotemark.
Go was a natural progression from chess due to its larger state space and higher branching factor.
In 2016, AlphaGo became world renowned after defeating Lee Sedol, 18-times world champion, thus becoming the first computer system achieving \textbf{superhuman perfomance} in the game of Go.
\footnotetext{Gary Kasparov, the legendary chess champion, was defeated by IBM chess-playing computer DeepBlue in 1997, marking the first superhuman computer performance in chess.}

However, a multitude of interesting studies in RL focuses on agents learning in \textbf{video games} or other game-like environments.
In Mnih et al.'s Atari DQN paper \cite{atari-dqn}, which is covered in this thesis, the DeepMind team proposes a method to train agents to play Atari video games exclusively from raw video input.
The study produced agents able to surpass human-level performance in Breakout, Enduro and Pong.
More importantly, it inspired a long chain of attempts of the research community to improve the original algorithm and beat the records it set.
% Starcraft II and Dota 2

This thesis aims to explore a challenging video game environment by analyzing the performance of existing algorithms in order to discover potentially underexplored areas in the design of AI agents.
We hope to outline some limitations of the current standards in order to benefit the further development of this field.

Toy problems such as playing video games from raw sensory input are the training ground which precedes real-world environments which require real-time intelligent control, such as self-driving cars.
Innovation in AI, as in all domains, begins at a small scale.

In order to accomplish our objective, we aim to build an agent framework that provides intuitive yet easily extensible tools, a solid testbed for experiments and means of data analysis.

Our choice for an environment for studying agents is a variant of popular arcade video game \emph{Pac-Man}, developed and released by Namco Ltd. in 1980.
This choice is motivated by two factors: the potential challenges it poses to AI and its fun and intuitive appeal for humans.
The game is challenging for AI agents because it involves balancing the goal of exploration and point acquisition with escaping different classes of enemies that chase the player.

This thesis can be roughly divided into a theoretical part which is compact and contained entirely within one exhaustive chapter, and a practical part, composed of an application chapter followed by an analysis of the obtained results.

\emph{Chapter \ref{chapter:background}, ``Background''} presents a comprehensive overview of the theory necessary to understanding reinforcement learning.
We start by introducing agent-based AI, then continue with a
section dedicated to understanding fundamental concepts in RL.
The second half of the chapter is dedicated to deep reinforcement learning, which can roughly be defined as combination of classic RL and neural networks.
Among the topics covered in the second half are convolutional neural networks (ConvNets) and some proeminent solution methods in deep RL.

\emph{Chapter \ref{chapter:practical}, ``Playing Pac-Man using Deep Q-learning''} states the problem and presents the chosen algorithms for our agents.
In particular, we present deep Q-networks (DQN) and two of its derviatives -- double DQN and prioritized experience replay.
Furthermore, we go in-depth over the structure of the framework and its implementation.

\emph{Chapter \ref{chapter:results}, ``Results and Evaluation''} presents our methods of measuring the performance of the agents and analyzes the obtained data of our various experiments.

In our ending chapter, \emph{Chapter \ref{chapter:conclusion}, ``Conclustion and Future Work''}, we sum up our methods and results and we identify future areas of improvement. Additionally, we try to predict the direction of RL research based on our conclusions.

\chapter{Background} \label{chapter:background}
\input{chapters/background}

\chapter{Playing Pac-Man using Deep Q-learning} \label{chapter:practical}
\input{chapters/approach}

\chapter{Results and Evaluation} \label{chapter:results}
\input{chapters/results}

\chapter{Conclusion and Future Work} \label{chapter:conclusion}
\input{chapters/conclusion}

\bibliography{references}

\end{document}