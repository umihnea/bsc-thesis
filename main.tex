\documentclass[12pt,twoside]{report}
\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage{amsmath,amssymb}
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm,bindingoffset=6mm]{geometry}
\setlength{\headheight}{15pt}

% hyperref
% source: timmurphy.org/2014/03/11/latex-table-of-contents-with-clickable-links/
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    urlcolor=red,
    linktoc=all
}

% math commands
\newcommand\given[1][]{\:#1\vert\:}
\DeclareMathOperator*{\argmax}{\arg\!\max}

% fancyhdr
\usepackage{fancyhdr}
\newcommand{\changefont}{%
    \fontsize{9}{11}\selectfont
}
\pagestyle{fancy}
\fancyhead[LE,RO]{\changefont \slshape \rightmark} %section
\fancyhead[RE,LO]{\changefont \slshape \leftmark} %chapter

% graphicx
\usepackage{graphicx}
\graphicspath{{images/}}

% biblatex
\usepackage[square,numbers]{natbib}
\bibliographystyle{unsrtnat}

\usepackage[nottoc]{tocbibind}

% For blank pages
% solution detailed here https://tex.stackexchange.com/a/36881
\usepackage{afterpage}
\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\begin{document}

\begin{titlepage}
    \center % Center everything on the page

    % University, Faculty and Specialization
    {\scshape\LARGE Babeş-Bolyai University Cluj-Napoca \par}
    \vspace{0.125cm}
    {\scshape\LARGE Faculty of Mathematics and Computer Science\par}
    \vspace{0.125cm}
    {\scshape\LARGE Specialization Computer Science\par}
    \vspace{5cm}

    % Title section
    {
        \bfseries
        \LARGE \uppercase{Diploma Thesis} \\[1.5cm]
        \huge Using Deep Q-networks to learn Pacman
    }\\[4cm]

    % Author and Supervisor
    \begin{flushleft}
        \Large
            \textbf{Supervisor}
            \vspace{0.2cm}\\
        \Large
            Lect. Ph.D. Radu D. Găceanu
            \vspace{0.125cm}\\
    \end{flushleft}

    \begin{flushright}
        \Large
            \textbf{Author}
            \vspace{0.2cm}\\
        \Large
            Mihnea Ungureanu
    \end{flushright}

    \vfill

    % Year
    {\center \large 2020}
\end{titlepage}

% A blank page is required
% The blankpage custom command is defined above 
\blankpage

% Romanian title page
\begin{titlepage}
    \center % Center everything on the page
    
    % University, Faculty and Specialization
    {\scshape\LARGE Universitatea Babeş-Bolyai Cluj-Napoca \par}
    \vspace{0.125cm}
    {\scshape\LARGE Facultatea de Matematică şi Informatică \par}
    \vspace{0.125cm}
    {\scshape\LARGE Specializarea Informatică Engleză \par}
    \vspace{5cm}
    
    % Title section
    {
        \bfseries
        \LARGE LUCRARE DE LICENȚĂ \\[1.5cm]
        \huge Folosind Deep Q-networks pentru a învăța Pacman
    }\\[4cm]
    
    % Author and Supervisor
    \begin{flushleft}
        \Large
            \textbf{Conducător științific}
            \vspace{0.2cm}\\
        \Large
            Lect. dr. Radu D. Găceanu
            \vspace{0.125cm}\\
    \end{flushleft}
    
    \begin{flushright}
        \Large
            \textbf{Absolvent}
            \vspace{0.2cm}\\
        \Large
            Mihnea Ungureanu
    \end{flushright}
    
    \vfill
    
    % Year
    {\center \large 2020}
    
\end{titlepage}

\blankpage

\chapter*{Abstract}
This thesis presents an autonomous agent capable of learning optimal behaviour in video game environments.

We focus on a variant of the famous arcade game \emph{Pacman}, due to its high potential as a research tool in the study of intelligent agents, as well as intuitive appeal for human players.

Despite seeming like a toy problem, learning to solve virtual environments provides insight into complex, real-world environments at an insignificant fraction of the cost of running realistic simulations.

The main purpose of this thesis is to implement, test and analyze results of different algorithms for autonomous learning and select the most performant approach at playing \emph{Pacman}.
A secondary aim is to test whether Pacman is a strong benchmark environment in \textbf{reinforcement learning (RL)} and search for challenges which would allow researchers to gain further insight into AI agents.

The theoretical part of this thesis starts by exploring classical \textbf{reinforcement learning} techniques, then transitions to modern approaches which leverage \textbf{artificial neural networks (ANNs)} as functional approximators for RL.

The practical part consists of multiple algorithm implementations, as well as a light framework for training, evaluating and benchmarking the performance of RL agents.
The provided agents include a replication of DeepMind’s \textbf{deep  Q-learning} strategy \cite{atari-dqn}, as well as more performant successors to the original paper: DDQN \cite{ddqn-paper} and Prioritized Experience Replay \cite{per-paper}.

The agents are implemented using PyTorch, a powerful Python machine learning library based on Torch. We also use the Atari toolkit from OpenAI Gym, which was designed for RL and provides interfaces to keep track of in-game score, lives and other indicators.

\hfill \break
This work is the result of my own activity. I have neither given nor received unauthorised assistance on this work.
\begin{flushright}
    Mihnea Ungureanu
\end{flushright}

% ToC placed between abstract and introduction
\tableofcontents


\chapter{Introduction}
Over the relatively recent years, \textbf{reinforcement learning (RL)} has gained immense popularity.
Stemming from the agent-based approach to artificial intelligence, RL is a unique paradigm within its field.

RL has a broad focus -- researchers have trained RL agents to solve a variety of tasks.
Among its applications was the AlphaGo family of Go programs \cite{ago, alpha-zero} which became world renowned after defeating Lee Sedol, 18-times world champion, thus becoming the first computer system achieving superhuman perfomance in the game of Go.

However, a multitude of interesting results in this field focuses on agents learning in video games or other game-like environments.
In \emph{Mnih et al.}'s 2013 \emph{Atari DQN} paper \cite{atari-dqn} (which is covered in this work), the DeepMind team proposes a method to train agents to play Atari video games exclusively from raw video input.

Despite seeming like a toy problem, learning to solve virtual environments from raw sensory input provides insight about complex, real-world environments which require real-time intelligent control, such as self-driving cars.

\chapter{Background}
\input{chapters/background}

\chapter{Our Approach}
\input{chapters/approach}

\chapter{Results and Evaluations}
\input{chapters/results}

\chapter{Conclustion and Future Work}
\input{chapters/conclusion}

\bibliography{references}

\end{document}