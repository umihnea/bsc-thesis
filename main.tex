\documentclass[12pt,twoside]{report}
\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage{amsmath,amssymb}
\usepackage[group-separator={,}]{siunitx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm,bindingoffset=6mm]{geometry}
\setlength{\headheight}{15pt}
\usepackage{sectsty}

% hyperref
% source: timmurphy.org/2014/03/11/latex-table-of-contents-with-clickable-links/
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    urlcolor=red,
    linktoc=all
}

% math commands
\newcommand\given[1][]{\:#1\vert\:}
\DeclareMathOperator*{\argmax}{\arg\!\max}

% fancyhdr
\usepackage{fancyhdr}
\newcommand{\changefont}{%
    \fontsize{9}{11}\selectfont
}
\pagestyle{fancy}
\fancyhead[LE,RO]{\changefont \slshape \rightmark} %section
\fancyhead[RE,LO]{\changefont \slshape \leftmark} %chapter

% graphicx
\usepackage{graphicx}
\graphicspath{{images/}}

% biblatex
\usepackage[square,numbers]{natbib}
\bibliographystyle{unsrtnat}

\usepackage[nottoc]{tocbibind}

% For blank pages
% solution detailed here https://tex.stackexchange.com/a/36881
\usepackage{afterpage}
\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\begin{document}

\begin{titlepage}
    \center % Center everything on the page

    % University, Faculty and Specialization
    {\scshape\LARGE Babeş-Bolyai University Cluj-Napoca \par}
    \vspace{0.125cm}
    {\scshape\LARGE Faculty of Mathematics and Computer Science\par}
    \vspace{0.125cm}
    {\scshape\LARGE Specialization Computer Science\par}
    \vspace{5cm}

    % Title section
    {
        \bfseries
        \LARGE \uppercase{Diploma Thesis} \\[1.5cm]
        \LARGE \uppercase{Using Deep Q-networks to learn Pac-Man}
    }\\[4cm]

    % Author and Supervisor
    \begin{flushleft}
        \Large
            \textbf{Supervisor}
            \vspace{0.2cm}\\
        \Large
            Lect. Ph.D. Radu D. Găceanu
            \vspace{0.125cm}\\
    \end{flushleft}

    \begin{flushright}
        \Large
            \textbf{Author}
            \vspace{0.2cm}\\
        \Large
            Mihnea Ungureanu
    \end{flushright}

    \vfill

    % Year
    {\center \large 2020}
\end{titlepage}

% A blank page is required
% The blankpage custom command is defined above 
\blankpage

% Romanian title page
\begin{titlepage}
    \center % Center everything on the page
    
    % University, Faculty and Specialization
    {\scshape\LARGE Universitatea Babeş-Bolyai Cluj-Napoca \par}
    \vspace{0.125cm}
    {\scshape\LARGE Facultatea de Matematică şi Informatică \par}
    \vspace{0.125cm}
    {\scshape\LARGE Specializarea Informatică Engleză \par}
    \vspace{5cm}
    
    % Title section
    {
        \bfseries
        \LARGE LUCRARE DE LICENȚĂ \\[1.5cm]
        \LARGE \uppercase{Folosirea de Deep Q-networks pentru a învĂȚa Pac-Man}
    }\\[4cm]
    
    % Author and Supervisor
    \begin{flushleft}
        \Large
            \textbf{Conducător științific}
            \vspace{0.2cm}\\
        \Large
            Lect. dr. Radu D. Găceanu
            \vspace{0.125cm}\\
    \end{flushleft}
    
    \begin{flushright}
        \Large
            \textbf{Absolvent}
            \vspace{0.2cm}\\
        \Large
            Mihnea Ungureanu
    \end{flushright}
    
    \vfill
    
    % Year
    {\center \large 2020}
    
\end{titlepage}

\blankpage

\chapter*{Abstract}
\small
The main purpose of this thesis is to implement, test and analyze the results of four fundamental algorithms in Deep Reinforcement Learning, in order to select the most performant approach at playing a variant of the famous arcade game \emph{Pac-Man}.
Our collection of implemented agents features a replication of DeepMind’s strategy, using Deep Q-networks (DQN), as well as two other performant successors to the original -- Double DQN, Prioritized Experience Replay (PER), and a hybrid of the latter two (Double PER).
A secondary aim is to build the necessary tools to study the strength of Pac-Man as a benchmark in Reinforcement Learning (RL), enabling others to collaborate and extend it.

% Despite seeming like a toy problem, learning to solve virtual environments provides insight into complex, real-world environments at an insignificant fraction of the cost of running realistic simulations.

The theoretical part of this thesis starts by exploring classical Reinforcement Learning techniques, then transitions to modern approaches which leverage Artificial Neural Networks (ANNs) to build more advanced agents, capable of solving more complex tasks.

The practical part consists of a Python framework for training, evaluating and benchmarking the performance of RL agents.
Our framework's main feature is the library of agents (software robots), actualizing the four aformentioned approaches.
Besides this, it also provides building blocks for training and evaluation logic, along with tools for visualizing and analyzing the results.
The main focus of the framework is on extensibility and ease of remote deployment.

For our evaluation, we start by training each agent by running game simulations for approximatively \num{120000} episodes per agent, tracking the score of each one for analysis.
After training is complete, each agent is again run for \num{2000} episodes to collect score data.
In our ranking, the top performing algorithms based on mean obtained score during evaluation are Double DQN and PER, scoring above the DQN baseline and several times over a random agent included for control.

Our contribution consists of reproducing and studying the performance of four fundamental algorithms in deep RL on the game of Pac-Man.
On one hand, providing our own rendition of the algorithms, based on their original specifications, and comparing our obtained performance with them, we study the reproducibility of the original papers.
On the other hand, our thesis introduces a novel perspective by focusing on depth over breadth.
Other works, including the original studies themselves, are explicitly focused on one-size-fits-all approaches and exclusively evaluate one algorithm across multiple games.
In our thesis, we reverse the perspective and instead rank several algorithms based on their performance in Pac-Man, which we believe is a suitable benchmark due to its highly dynamic nature.

We use a suite of high-performance libraries. The neural networks powering the agents are implemented using PyTorch, a powerful Python machine learning library based on Torch. The Atari toolkit from OpenAI Gym provides our Pac-Man environment, and we analyze our results using Jupyter Notebook, a well-established Python data science tool.

\hfill \break
This work is the result of my own activity. I have neither given nor received unauthorised assistance on this work.
\begin{flushright}
    Mihnea Ungureanu
\end{flushright}


% ToC placed between abstract and introduction
\tableofcontents


\chapter{Introduction}
Over the relatively recent years, \textbf{reinforcement learning (RL)} has gained immense popularity.
Stemming from the agent-based approach to artificial intelligence, it is a unique paradigm within the field of machine learning.
RL directly focuses on building agents that make optimal decisions in an environment in order to maximize a notion of cumulative reward.
RL has a broad focus and researchers have trained RL agents to solve a variety of tasks.

One of the field's most notable achievments is DeepMind's \textbf{AlphaGo} family of Go-playing programs \cite{ago, alpha-zero}.
Beating humans at Go was widely acknowledged as the next frontier in game-playing AI after the defeat of Gary Kasparov in 1997\footnotemark.
Go was a natural progression from chess due to its larger state space and higher branching factor.
In 2016, AlphaGo became world renowned after defeating Lee Sedol, 18-times world champion, thus becoming the first computer system achieving \textbf{superhuman perfomance} in the game of Go.
\footnotetext{Gary Kasparov, the legendary chess champion, was defeated by IBM chess-playing computer DeepBlue in 1997, marking the first superhuman computer performance in chess.}

However, a multitude of interesting studies in RL focuses on agents learning in \textbf{video games} or other game-like environments.
In Mnih et al.'s Atari DQN paper \cite{atari-dqn}, which is covered in this thesis, the DeepMind team proposes a method to train agents to play Atari video games exclusively from raw video input.
The study produced agents able to surpass human-level performance in Breakout, Enduro and Pong.
More importantly, it inspired a long chain of attempts of the research community to improve the original algorithm and beat the records it set.
% Starcraft II and Dota 2

This thesis aims to explore a challenging video game environment by analyzing the performance of existing algorithms in order to discover potentially underexplored areas in the design of AI agents.
We hope to outline some limitations of the current standards in order to benefit the further development of this field.

Toy problems such as playing video games from raw sensory input are the training ground which precedes real-world environments which require real-time intelligent control, such as self-driving cars.
Innovation in AI, as in all domains, begins at a small scale.

In order to accomplish our objective, we aim to build an agent framework that provides intuitive yet easily extensible tools, a solid testbed for experiments and means of data analysis.

Our choice for an environment for studying agents is a variant of popular arcade video game \emph{Pac-Man}, developed and released by Namco Ltd. in 1980.
This choice is motivated by two factors: the potential challenges it poses to AI and its fun and intuitive appeal for humans.
The game is challenging for AI agents because it involves balancing the goal of exploration and point acquisition with escaping different classes of enemies that chase the player.

This thesis can be roughly divided into a theoretical part which is compact and contained entirely within one exhaustive chapter, and a practical part, composed of an application chapter followed by an analysis of the obtained results.

\emph{Chapter \ref{chapter:background}, ``Background''} presents a comprehensive overview of the theory necessary to understanding reinforcement learning.
We start by introducing agent-based AI, then continue with a
section dedicated to understanding fundamental concepts in RL.
The second half of the chapter is dedicated to deep reinforcement learning, which can roughly be defined as combination of classic RL and neural networks.
Among the topics covered in the second half are convolutional neural networks (ConvNets) and some proeminent solution methods in deep RL.

\emph{Chapter \ref{chapter:practical}, ``Playing Pac-Man using Deep Q-learning''} states the problem and presents the chosen algorithms for our agents.
In particular, we present deep Q-networks (DQN) and two of its derviatives -- double DQN and prioritized experience replay.
Furthermore, we go in-depth over the structure of the framework and its implementation.

\emph{Chapter \ref{chapter:results}, ``Results and Evaluation''} presents our methods of measuring the performance of the agents and analyzes the obtained data of our various experiments.

In our ending chapter, \emph{Chapter \ref{chapter:conclusion}, ``Conclustion and Future Work''}, we sum up our methods and results and we identify future areas of improvement. Additionally, we try to predict the direction of RL research based on our conclusions.

\chapter{Background} \label{chapter:background}
\input{chapters/background}

\chapter{Playing Pac-Man using Deep Q-learning} \label{chapter:practical}
\input{chapters/approach}

\chapter{Results and Evaluation} \label{chapter:results}
\input{chapters/results}

\chapter{Conclusion and Future Work} \label{chapter:conclusion}
\input{chapters/conclusion}

\bibliography{references}

\end{document}