\documentclass[12pt,twoside]{report}
\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage{amsmath,amssymb}
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm,bindingoffset=6mm]{geometry}
\setlength{\headheight}{15pt}

% hyperref
% source: timmurphy.org/2014/03/11/latex-table-of-contents-with-clickable-links/
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    urlcolor=red,
    linktoc=all
}

% math commands
\newcommand\given[1][]{\:#1\vert\:}
\DeclareMathOperator*{\argmax}{\arg\!\max}

% fancyhdr
\usepackage{fancyhdr}
\newcommand{\changefont}{%
    \fontsize{9}{11}\selectfont
}
\pagestyle{fancy}
\fancyhead[LE,RO]{\changefont \slshape \rightmark} %section
\fancyhead[RE,LO]{\changefont \slshape \leftmark} %chapter

% graphicx
\usepackage{graphicx}
\graphicspath{{images/}}

% biblatex
\usepackage[square,numbers]{natbib}
\bibliographystyle{unsrtnat}

\usepackage[nottoc]{tocbibind}

% For blank pages
% solution detailed here https://tex.stackexchange.com/a/36881
\usepackage{afterpage}
\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}

\begin{document}

\begin{titlepage}
    \center % Center everything on the page

    % University, Faculty and Specialization
    {\scshape\LARGE Babeş-Bolyai University Cluj-Napoca \par}
    \vspace{0.125cm}
    {\scshape\LARGE Faculty of Mathematics and Computer Science\par}
    \vspace{0.125cm}
    {\scshape\LARGE Specialization Computer Science\par}
    \vspace{5cm}

    % Title section
    {
        \bfseries
        \LARGE \uppercase{Diploma Thesis} \\[1.5cm]
        \huge Using Deep Q-networks to learn Pacman
    }\\[4cm]

    % Author and Supervisor
    \begin{flushleft}
        \Large
            \textbf{Supervisor}
            \vspace{0.2cm}\\
        \Large
            Lect. Ph.D. Radu D. Găceanu
            \vspace{0.125cm}\\
    \end{flushleft}

    \begin{flushright}
        \Large
            \textbf{Author}
            \vspace{0.2cm}\\
        \Large
            Mihnea Ungureanu
    \end{flushright}

    \vfill

    % Year
    {\center \large 2020}
\end{titlepage}

% A blank page is required
% The blankpage custom command is defined above 
\blankpage

% Romanian title page
\begin{titlepage}
    \center % Center everything on the page
    
    % University, Faculty and Specialization
    {\scshape\LARGE Universitatea Babeş-Bolyai Cluj-Napoca \par}
    \vspace{0.125cm}
    {\scshape\LARGE Facultatea de Matematică şi Informatică \par}
    \vspace{0.125cm}
    {\scshape\LARGE Specializarea Informatică Engleză \par}
    \vspace{5cm}
    
    % Title section
    {
        \bfseries
        \LARGE LUCRARE DE LICENȚĂ \\[1.5cm]
        \huge Folosind Deep Q-networks pentru a învăța Pacman
    }\\[4cm]
    
    % Author and Supervisor
    \begin{flushleft}
        \Large
            \textbf{Conducător științific}
            \vspace{0.2cm}\\
        \Large
            Lect. dr. Radu D. Găceanu
            \vspace{0.125cm}\\
    \end{flushleft}
    
    \begin{flushright}
        \Large
            \textbf{Absolvent}
            \vspace{0.2cm}\\
        \Large
            Mihnea Ungureanu
    \end{flushright}
    
    \vfill
    
    % Year
    {\center \large 2020}
    
\end{titlepage}

\blankpage

\chapter*{Abstract}
In this paper, we use \textbf{reinforcement learning (RL)} methods to train an autonomous agent to behave optimally in a video game environment.
We choose to solve \emph{Pacman}, for its simple mechanics but potentially complex situations that can emerge.
In order to train our agent, we use a method similar to the one described in the Atari DQN paper \cite{atari-dqn}.
We elaborate on the theoretical framework on which the paper is built.
Finally, we describe our working implementation of the algorithm and document the results.


% ToC placed between abstract and introduction
\tableofcontents


\chapter{Introduction}
Over the relatively recent years, \textbf{reinforcement learning (RL)} has gained immense popularity.
Stemming from the agent-based approach to artificial intelligence, RL is a unique paradigm within its field.

RL has a broad focus -- researchers have trained RL agents to solve a variety of tasks.
Among its applications was the AlphaGo family of Go programs \cite{ago, alpha-zero} which became world renowned after defeating Lee Sedol, 18-times world champion, thus becoming the first computer system achieving superhuman perfomance in the game of Go.

However, a multitude of interesting results in this field focuses on agents learning in video games or other game-like environments.
In \emph{Mnih et al.}'s 2013 \emph{Atari DQN} paper \cite{atari-dqn} (which is covered in this work), the DeepMind team proposes a method to train agents to play Atari video games exclusively from raw video input.

Despite seeming like a toy problem, learning to solve virtual environments from raw sensory input provides insight about complex, real-world environments which require real-time intelligent control, such as self-driving cars.

\chapter{Background}
\input{chapters/background}

\chapter{Our Approach}
\input{chapters/approach}

\chapter{Results and Evaluations}
\input{chapters/results}

\chapter{Conclustion and Future Work}
\input{chapters/conclusion}

\bibliography{references}

\end{document}