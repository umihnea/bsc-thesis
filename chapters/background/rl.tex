\textbf{Reinforcement learning (RL)} is an area of machine learning that epitomizes the agent-based approach.
RL is unique within its field as it is directly focused on goal-directed learning from agent-environment interaction \cite{rlai}.
At the core of RL lays the reward function (or signal), which perfectly describes goals established by the problem.

In the following section, we start by summarizing finite MDPs -- the mathematical underpinning for reinforcement learning, while also familiarizing with the the notation used throughout this thesis.

In Section \ref{rl:dp} we cover the first solution technique: using dynamic programming in a known, finite MDP to find its optimal solution.
In Section \ref{rl:mc}, we generalize this method to solve unknown MDPs -- the complete formulation of a RL problem -- using Monte Carlo learning.
In Section \ref{rl:td}, we touch on temporal-difference (TD) learning. Finally, we present Q-learning -- a fundamental algorithm we reference to throughout this thesis -- in Section \ref{rl:q-learning}.

\subsection{Components and Notation}
Introduction into reinforcement learning supposes familiarity with the paradigm's simple-but-effective mathematical framework.
We present the theory behind Markov decision processes (MDPs) and then we shift our attention to explaing the components of a reinforcement learning agent.

\subsubsection{Finite Markov Decision Processes}
Finite Markov decision processes are a formalization of the reinforcement learning problem.
Any problem which can be represented as a finite MDP can be solved using a RL technique.

The feedback loop is central to understanding the reinforcement learning dynamic.
This is just a reiteration of the contents in our previous chapter:
an \textbf{agent} acts upon an \textbf{environment}, which reacts according to its set of governing rules.
Each iteration takes place at a discrete time step \(t = 0,1,2,\dots \) (although there are continuous variants of a Markov process, we are not concerned with them).

\begin{figure}[ht]
    \centering
    \caption{Feedback Loop. (Partial reproduction from \cite{rlai})}
    \vspace*{0.2cm}
    \includegraphics[width=0.5\textwidth]{agent-env-fig}
\end{figure}

\begin{enumerate}
    \item The agent is in state \(s\) and picks action \(a\).
    \item It receives a reward \(r\) for its action. This (immediate) reward signal is problem-defined and quantifies the agent’s goal.
    \item The environment reacts, sending the agent into the next state \(s'\).
\end{enumerate}

A \textbf{Markov decision process (MDP)} is defined as a tuple \(\langle S, A, P, R, \gamma \rangle\), in which:
\begin{enumerate}
    \item \(S\) is the finite set of all states.
    \item \(A\) is the finite set of all possible actions.
    \item \(P\) is the state transition probability function, where \(P_a(s, s')\) denotes the probability of starting in state \(s\) and ending up in state \(s'\) by taking action \(a\).
    \item \(R\) is the reward function, where \(R_a(s, s')\) is a value in \(\mathbb{R}\) and represents the reward received for starting in state \(s\) and ending up in state \(s'\) by taking action \(a\).
    \item \(\gamma\) is the discount factor (used when computing the \textbf{return}), \(\gamma \in [0, 1]\).
\end{enumerate}

The \textbf{state} refers to the internal state of the agent \footnote{Sometimes we may also refer to the state of the environment, but context will clarify. Environment state and agent state only completely match in full observability environments.}.
A state \(S_t\) contains relevant information from the environment at a given time step \(t\).
A key point in Markov processes is that a correctly formulated state captures all the relevant information and removes the need of explicitly memorizing state history.
This is known as the \textbf{Markov property} \cite{silver-lectures}.

A \textbf{policy \(\pi\)} completely characterizes an agent’s behaviour.
It maps perceived states (observations of the environment) to the actions to be taken based on those states \cite{rlai}.
Policies can be deterministic (i.e., ``if \verb|p| then \verb|q|'' rules) or stochastic.
A \textbf{stochastic policy}, denoted \(\pi(a \given s)\), is a probability distribution over actions, given a state.

The \textbf{reward} models the problem-defined goals as a scalar that can be associated with each state transition.
``The reward signal is the primary basis of altering the policy'' \cite{rlai}.
The assertion that we can completely and correctly model all goals using reward functions is central to the field of RL.
This is called the \textbf{reward hypothesis} and is formulated below:
\begin{quotation}
    That all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (called reward). (from Chapter 3.2 of \emph{Reinforcement Learning. An Introduction} \cite{rlai})
\end{quotation}

The \textbf{return} \(G_t\) is the \textbf{cumulative reward} obtained by the agent starting from time step \(t\).
This is what a reinforcement learning system is supposed to maximize.
There are multiple mathematical models used to represent the return.
In the expression below, we use an infinite-horizon sum with discounting.

\begin{equation} \label{eqn:return}
    G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k = 0}^{\infty}{\gamma^{k} R_{t + k + 1}}    
\end{equation}

The purpose of the \textbf{discount factor} $\gamma$ is dual.
Discounting is, \emph{intuitively}, a way to control how much the agent cares about future rewards.
It incorporates a model of \emph{human thought}: immediate reward is preferred to delayed reward.
A value of $\gamma$ in $(0, 1)$ is also \emph{mathematically} convenient, as it serves as a guarantee for convergence in infinite horizon sums such as the one expressed in (\ref{eqn:return}).

\subsubsection{Value Functions} \label{rl:value-func}
The \textbf{state-value function} for policy \(\pi\) is denoted by \(V^{\pi}(s)\) and measures how good each state is, with regard to the long-term potential of that state.
Whereas rewards characterize the immediate value of a state, the value function measures its long-term desirability \cite{rlai}, under the given policy.

\begin{equation} \label{eqn:value}
    V^{\pi}(s) = \mathbb{E}_{\pi}\{ G_t \given S_t = s \}
\end{equation}

In Equation \ref{eqn:value}, $\mathbb{E}$ represents the expected value of a random variable. Here we are interested in the expected value of the return, or simply, the \textbf{expected return}.
By subscripting with $\pi$, we mean that the computation of the expected value is conditional on following policy $\pi$.
We will maintain this notation in the expressions below.

The \textbf{action-value function} \(Q^{\pi}(s, a)\), also known as the Q-value function, measures the desirability of a state-action pair.
More explicitly, it gives the expected return of starting in state $s$ and taking action $a$, then continuing to follow policy $\pi$.

\begin{equation}
    Q^{\pi}(s, a) = \mathbb{E}_{\pi}\{ G_t \given S_t = s, A_t = a \}
\end{equation}

Action-value functions are especially important in \textbf{model-free methods} (see \emph{Models} in Section \ref{rl:model}), which are predominantly the focus of this thesis.
In model-based algorithms, the value function $v$ can uniquely determine a policy: we can simply pick the best reward-state pair.
This naturally requires knowing the full dynamics of the MDP (knowing all successor states of $s$).

However, for many tasks, especially those relevant in the real-world, it is impractical (if not impossible) to know or to model the environment.
Keeping track of both \emph{states and actions} allows model-free learning algorithms to construct policies without knowledge of environment dynamics.

\subsubsection{Optimal Value Functions}

There are optimal variants of the same value functions.
In order to understand the concept of \textbf{optimality}, we need to define an \textbf{ordering} between policies.
Considering $\pi$ and $\pi'$, we say that $\pi' \geq \pi$ if $V^{\pi'}(s) \geq V^{\pi}(s)$ for every $s \in S$.
Using this definition, we can say that there exists a $\pi^{*}$ which is the \textbf{optimal policy}, being greater than or equal to all other possible policies.

The \textbf{optimal value function $V^*(s)$} represents the maximum value obtained in state $s$, over all policies, as briefly summed up the relation below.

\begin{equation} \label{eqn:opt-value-fun}
    V^*(s) = \max_{\pi}{V^{\pi}(s)}
\end{equation}

Similarly, \textbf{optimal action-value function $Q^*(s, a)$} is the maximum value of being in state $s$ and taking action $a$, considered over all policies.

\begin{equation} \label{eqn:opt-Q-fun}
    Q^*(s) = \max_{\pi}{Q^{\pi}(s, a)}
\end{equation}

Estimating value functions is the foundation of a multitude of solution methods in RL.
A simple example can be seen in Monte Carlo methods, where the value of a state $s$ is approximated by averaging over multiple trajectories starting from $s$.

\subsubsection{Models} \label{rl:model}
A \textbf{model} (of the environment) allows the agent to plan and make predictions of its environment.
Some algorithms focus explicitly on learning a model and use it for \textbf{planning}.
This approach is called \textbf{model-based}.
In a model-based method, an agent can query the model to simulate what would happen with the environment before actually choosing an action.
Approaches without a model are called \textbf{model-free}. Model-free agents are ``explicitly trial-and-error learners'' \cite{rlai}.

\begin{figure}[ht]
    \caption{A way of classifying RL methods based on whether they have a value function, policy or model. (Reproduced from David Silver's lectures. \cite{silver-lectures})}
    \vspace*{0.2cm}
    \centering
    \includegraphics[width=0.5\textwidth]{silver-venn}
\end{figure}

\subsection{Bellman Equations} \label{rl:bellman}
The \textbf{Bellman equations} are a set of relations conceived by Richard Bellman in the 1950s, in the context of optimal control.
In a reinforcement learning context, they interrelate the state-value and action-value functions and provide recursive definitions for them.
They form the basis for solving an MDP, whether known or unknown.

Equation \ref{eqn:bellman-expectation} states the \textbf{Bellman expectation equation}.
The key idea behind this class of Bellman equations is that they express the value of a state as expected sum of immediate reward of being in that state,  plus discounted value of successor state.
The transformation becomes clear if we use the return-based exprimation of value function $V^{\pi}(s)$ in Equation \ref{eqn:value} and reorganize its terms:

\begin{equation} \label{eqn:bellman-expectation}
\begin{aligned}
    V^{\pi}(s)
        &= \mathbb{E}_{\pi}\{ G_t \given S_t = s \} \\
        &= \mathbb{E}_{\pi}\{ R_{t+1} + \gamma R_{t+2} + \dots \given S_t = s \} \\
        &= \mathbb{E}_{\pi}\{ R_{t+1} + \gamma V^{\pi}(S_{t+1}) \given S_{t} = s \} \\
\end{aligned}
\end{equation}

In Figure \ref{fig:expectation-backup}, the equation is represented as a backup tree\footnote{A \textbf{backup tree} or \textbf{update tree} models the way in which state values are updated. This visualization convention is proposed in and used throughout ``Reinforcement Learning: An Introduction'' \cite{rlai}.}, where each open circle represents a state, while closed cicles represent state-action pairs.

In state $s$, the agent has a set of available actions according to its policy $\pi$.
After the agent chooses action $a$, the environment responds with reward $r$ and throws the agent into the next state  $s'$ -- one of multiple possible states according to environment dynamics.

Equation \ref{fig:expectation-backup} computes the value of state $s$ by \emph{averaging} over all possible trajectories, \emph{weighing} each by its probability of occuring.

\begin{figure}[ht]
    \centering
    \caption{Bellman expectation backup.} \label{fig:expectation-backup}
    \vspace*{0.2cm}
    \includegraphics[width=0.4\textwidth]{bellman-expectation-backup}
\end{figure}

In cases in which the MDP is \emph{known}, we can leverage the fact that we know the transition probabilities $P$ to \emph{directly compute} the expectation in \ref{eqn:bellman-expectation}.
\begin{equation}
    \begin{aligned}
        V^{\pi}(s)
            &= \mathbb{E}_{\pi}\{ R_{t+1} + \gamma V^{\pi}(S_{t+1}) \given S_{t} = s \} \\
            &= \sum_{a \in A}{
                \pi(a \given s) ( R(s, a) + \gamma \sum_{s' \in S}{
                    P_a(s, s') V^{\pi}(s')
                })
            }
    \end{aligned}
\end{equation}

There exists another relation with important applications in RL -- the \textbf{Bellman optimality equation}.
One way of expressing the optimality equation, provided in (\ref{eqn:bellman-optimality}), simply states the fact that the optimal value of a state must be the value of the best possible action from that state \cite{rlai}.
This is an example of interdependence of the two value functions, although it is important to mention that we can express $q_{*}$ in terms of itself as well.

\begin{equation} \label{eqn:bellman-optimality}
    \begin{aligned}
        v_*(s) = \max_{a \in A} q_{*}(s, a)
    \end{aligned}
\end{equation}

The optimality equation is non-linear, but we cover approximating solutions using iterative methods in our dynamic programming section at \ref{rl:dp}, specifically to solve planning problems (which require finding the optimal policy).

\subsection{Dynamic Programming} \label{rl:dp}
\textbf{Dynamic programming (DP)} methods are an important theoretical starting point for reinforcement learning methods. DP problems require full knowledge of an MDP and use general methods to compute optimal solutions.

While being theoretically useful, there are key bottlenecks which prevent DP from achieving practicality:

\begin{enumerate}
    \item DP represents a \textbf{subset} of the reinforcement learning problem, as it requires full knowledge of an MDP and, thus, is not suitable in unknown environments.
    \item DP is \textbf{computationally expensive} as it searches for optimal solutions over the entire state space (suffers from the ``curse of dimensionality'' \cite{rlai}).
    \item DP cannot be applied with \textbf{continuous} spaces, unless problems satisfy additional criteria \cite{rlai}.
\end{enumerate}

There are two key problems that can be solved using DP in a fully known MDP using the Bellman equations.

\textbf{Policy evaluation}, also reffered to as \emph{prediction}, starts with a given policy $\pi$ and is concerned with finding its value function $v_{\pi}$.
In theory, the value of each state can be computed by solving a system of Bellman equations, one for each state.
However, the computation required is impractical for most problems.
A more suitable method is \emph{iterative policy evaluation}, which uses equation \ref{eqn:bellman-expectation} iteratively, starting from an arbitraty value function $v_0$ and eventually converging to $v_{\pi}$.

\textbf{Control} (or \textbf{planning}) is, in constrast, concerned with finding the optimal policy $\pi^{*}$ starting from an arbitraty point in the space of all policies.

Planning can be done using \emph{policy iteration}.
At every iteration $k$, this method evaluates the given policy $\pi$ to find its value function $v_{\pi}$ (solving the above problem).
Then, it constructs a new policy $\pi'$ which acts \emph{greedily} with regard to $v_{\pi}$.
This is proven to eventually converge to $\pi^*$ as $k \to \infty$ \cite{rlai}.

\begin{figure}[h]
    \caption{An illustration of policy iteration. (From David Silver's lectures. \cite{silver-lectures})}
    \centering
    \includegraphics[width=0.4\textwidth]{silver-policy-iteration}
\end{figure}

Using policy iteration can be extremely expensive, as the method adds an additional layer of computation on top of already costly policy evalutation.
The idea of \textbf{value iteration} comes from collapsing the evaluation step with the policy construction step, by applying ``one sweep'' policy iteration -- only updating each value once at every step.

\subsubsection{Generalized Policy Iteration (GPI)}
As we have seen so far, policy iteration consists of two simultaneous, interacting processes \cite{rlai}: policy evaluation and policy improvement.
Different algorithms interrelate the two phases in different ways, e.g. value iteration performs one sweep of policy evaluation while vanilla policy iteration goes through to the end.

\textbf{Generalized policy iteration} is a broad framework specifying only that policy evaluation and policy iteration are used alongside each other, abstracting the exact details of their interaction.
Most reinforcement learning algorithms can be described by GPI at the high-level.

\clearpage

\subsection{Monte Carlo Learning} \label{rl:mc}
In reinforcement learning, Monte Carlo is a family of methods that learn from complete episodes of experience by sampling average returns.
The term “Monte Carlo” is broadly used in mathematics to mean any evaluation method that involves a significant random component \cite{rlai}.

\begin{figure}[ht]
    \caption{Backup tree for Monte Carlo, highlighting a sample episode up to its terminal state. (From David Silver's lectures. \cite{silver-lectures})}
    \centering
    \includegraphics[width=0.5\textwidth]{montecarlo-backup.png}
\end{figure}

Monte Carlo approximates the expected return of any given state by averaging over the set of \emph{episodic returns} for that state.
For this, we interact with the environment over multiple episodes, with the condition that every episode \emph{eventually terminates}, independently of the actions taken by the agent.
Eventual termination is necessary for all returns to be well-defined.

Monte Carlo introduces one of the missing pieces required to solve model-free problems -- \textbf{sampling}.
Here we describe a simplified, general version of the algorithm to better illustrate the approach.
Let each state hold a variable $count(s)$, counting the number of first-visits (or every-visits) over the set of all sampled episodes, as well as a variable $sum(s)$, summing the values of returns over the same set of samples.
The value of a state $v(s)$ can then be approximated as the mean over all obtained returns, $sum(s) / count(s)$.
Given the \emph{law of large numbers}, $v(s)$ will eventually converge to the correct values as the number of episodes approaches infinity.

\subsubsection{Monte Carlo Control}
Monte Carlo addresses the complete reinforcement learning problem, and more precisely, it solves \textbf{model-free prediction}.
It only requires episodes of experience to learn the optimal solution (in contrast to DP, which requires a fully known model of the environment).

Since Monte Carlo is a viable strategy for policy evaluation in unknown environments, we can apply it to \textbf{control problems}.
In order to do this, we use the General Policy Iteration (GPI) framework, drawing upon what we discussed in our dynamic programming section.

\begin{figure}[h]
    \includegraphics[width=5cm]{montecarlo-gpi.png}
    \centering
    \caption{Monte Carlo control using the GPI framework. (From \cite{rlai})}
\end{figure}

Applying this pattern, we can use Monte Carlo estimation as the prediction step to approximate the optimal policy, thus obtraining Monte Carlo control.
Since we this is a control algorithm for unknown environments, we are required to learn the action-value function, as mentioned in our section on value functions.
The policy improvement step is simply done by choosing the action yielding the maximum value.
\begin{equation}
    \pi(s) = \argmax_{a \in A}{Q(s, a)}
\end{equation}

\subsubsection{Exploration and Soft Policies}

Besides the termination requirement and using the action-value function, Monte Carlo control will not converge to any useful solution unless efficient \textbf{exploration} of the state space is guaranteed.
For this purpose, we introduce the notion of a \textbf{soft policy}.

Soft policies guarantee that we do not permanently exclude any state from our search, or -- in other words -- that $\pi (s, a) > 0$ for all $s \in S$ and $a \in A$.
\textbf{$\varepsilon$-greedy policies} are commonly used for this purpose.
Under an $\varepsilon$-greedy policy, $\varepsilon \in (0, 1]$ is the probability of choosing a random action in any state instead of acting greedily.

\subsection{Temporal-Difference Learning} \label{rl:td}
\textbf{Temporal-difference (TD) learning} is a class of model-free algorithms that are able to learn from incomplete episodes.
Compared to Monte Carlo, TD methods (being model-free) similarly learn by sampling from the environment, but differ through the fact that they do not require complete episodes of experience.

There exist advanced generalizations of the simple TD algorithm, such as TD($\lambda$), but they are outside the scope of this thesis.
Here we will only cover the case of one-step TD learning, known in \cite{rlai} as TD(0).

\subsubsection{Bootstrapping}
Similar to the dynamic programming approach, TD updates the value function based on previous estimations of itself.
This is called \textbf{bootstrapping} and can be intuitively described as the idea of \emph{learning a guess from a guess} \cite{rlai}.

\begin{figure}[h]
    \includegraphics[width=0.7\textwidth]{td-mc-backups.png}
    \centering
    \caption[Caption]{Backup tree for one update step of TD(0) next to one update step of every-visit Monte Carlo.} \label{fig:td-mc-update-steps}
\end{figure}

In Figure \ref{fig:td-mc-update-steps}, an \emph{update step} is not a time-step but instead refers to an iteration of each algortihm.
In the case of TD(0), the terms do correspond, as the update can be performed after each time-step.
For Monte Carlo, the update is run only after the end of each episode.
The concept can be generalized to \textbf{$n$-step bootstrapping} (updating after $n$ steps instead of one).

Temporal-difference solves model-free prediction, the same problem as Monte Carlo.
In this section we contrast the two approaches and establish the definition of the TD target and the TD error.

We consider a Monte Carlo approach where we update $V^{\pi}(s)$ towards the return $G_t$ softly at the end of each episode, considering a learning rate of $\alpha \in (0, 1)$.
This Monte Carlo backup operation is given by Equation \ref{eqn:td-mc} \cite{rlai}.

\begin{equation} \label{eqn:td-mc}
    V(S_t) \leftarrow  V(S_t) + \alpha[G_t - V(S_t)]
\end{equation}

Observe how in Monte Carlo, we update at the end of an episode and so we know the return $G_t$ exactly -- this is our \emph{ground truth}.
In TD, the rewards themselves provide a stable ground, but the values are adjusted towards our \emph{best guess} (the value from our previous iteration).

\begin{equation} \label{eqn:td-td}
    V(S_t) \leftarrow  V(S_t) + \alpha[
        \overbrace{\underbrace{R_{t+1} + \gamma V(S_{t+1})}_\text{TD target} - V(S_t)}^\text{TD error}
    ]
\end{equation}

Equation \ref{eqn:td-td} describes the update step in temporal-difference learning.
We define the \textbf{TD target} according to the Bellman expectation equation -- $\delta = R_{t+1} + \gamma V(S_{t+1})$.
After each step of experience, the current value is adjusted softly towards the \textbf{TD error} which is simply $\delta - V(S_t)$.

\textbf{TD control}, like Monte Carlo control, uses GPI with TD as the prediction step.
Control algorithms are split into two distinct classes: on-policy and off-policy algorithms.
We explain the difference between the two, then focus on the off-policy TD case (Q-learning, in Section \ref{rl:q-learning}).

\subsection{On-Policy and Off-Policy Control}
In control algorithms, we can distinguish two different types of policies.
\begin{enumerate}
    \item The \textbf{target policy} is the policy which the algorithm estimates or optimizes -- this type, whether explicitly or implicitly, is present in all RL algorithms.
    \item The \textbf{behaviour policy} (often denoted by $\mu$) is the policy effectively followed by the agent, i.e. it is directly used to choose actions in the environment.
\end{enumerate}

The class of algorithms which uses a behaviour policy $\mu$ in order to learn about a \emph{separate} target policy $\pi$ is called \textbf{off-policy}.
Off-policy agents try to learn optimal behaviour by observation of exploratory behaviour.

The alternative strategy is \textbf{on-policy} learning, in which the learned policy coincides with the behaviour policy.
On-policy learning agents learn ``on the job'' \cite{silver-lectures}, i.e. evaluate a policy by acting according to that policy.

Off-policy approaches are suitable for learning from knowledge generated by human experts or by other agents, including non-intelligent agents.
They are more desirable \emph{in general}, as on-policy agents tend to learn suboptimal behaviours in order to counteract the effects of exploration (as can be seen from the \emph{Example 6.6. Cliff Walking}, given in \cite{rlai}).

Despite being generally more useful, off-policy learning introduces \textbf{additional complexity} because it needs additional guarantees.
Off-policy learning assumes that behaviour policy $\mu$ has \textbf{coverage} of the target policy $\pi$ \cite{rlai}.
Coverage implies that $\mu$ keeps visiting all state-action pairs that would be visited by $\pi$.
In other words, we can say that $\pi(s, a) > 0$ implies that $\mu(s, a) > 0$.

% todo: This would be a good place to plug-in and explain Cliff Walking in detail.

TD learning contains strategies aligning with both categories: Sarsa is an on-policy TD control algorithm, while Q-learning is its off-policy counterpart.

\subsection{Q-learning} \label{rl:q-learning}
\textbf{Q-learning} \cite{Watkins1992} an off-policy TD learning algorithm considered one of the early breakthroughs of RL.
Its rather non-descriptive name comes from the action-value function $Q$ (which itself stands for ``quality'').

% todo: Consider whether this chapter should be "Sarsa and Q-learning" where we present both algorithms instead of only Q-learning.

Q-learning learns an optimal policy by following an $\varepsilon$-greedy exploratory policy.
Both policies are directly based on the learned action-value function.
The principle underneath, however, can be adapted to learn from any other policy as long as the coverage property is respected.
Q-learning achieves coverage, since a greedy policy is ``included'' into its $\varepsilon$-greedy counterpart.

At each step, Q-learning bootstraps using the \emph{maximum} action-value over all successor states in order to better estimate the current state value.
The update rule is represented in Figure \ref{fig:q-learning-backup}.
The arc over the set of successor states represents the maximization (greedy) operation.

\begin{figure}[h]
    \centering
    \includegraphics[width=3cm]{q-learning-backup.png}
    \caption{Q-learning backup diagram.}
    \label{fig:q-learning-backup}
\end{figure}

The same rule is mathematically represented in Equation \ref{eqn:q-learning-update-rule}.
It follows the TD learning pattern presented in Equation \ref{eqn:td-td}. Here, our \textbf{TD target} uses the maximum successor Q-value -- $\delta = R_{t+1} + \gamma \max_{a \in A}{Q(S_{t+1}, a)}$.


\begin{equation} \label{eqn:q-learning-update-rule}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [
        R_{t+1} + \gamma \max_{a \in A}{Q(S_{t+1}, a)} - Q(S_t, A_t)
    ]
\end{equation}
 
\clearpage